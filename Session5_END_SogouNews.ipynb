{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Session5_END_SogouNews_Text_Classification.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNS2aVQiQw49kHfXFPED2VB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NLP-END3/Session5/blob/main/Session5_END_SogouNews.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXAxWXNYcgQ_"
      },
      "source": [
        "import torch\n",
        "from torchtext.datasets import SogouNews"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LpaE6-OndG5M",
        "outputId": "23fdf2f9-7380-46fd-b638-ac16e28c501a"
      },
      "source": [
        "help(SogouNews)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on function SogouNews in module torchtext.datasets.sogounews:\n",
            "\n",
            "SogouNews(root='.data', split=('train', 'test'))\n",
            "    SogouNews dataset\n",
            "    \n",
            "    Separately returns the train/test split\n",
            "    \n",
            "    Number of lines per split:\n",
            "        train: 450000\n",
            "    \n",
            "        test: 60000\n",
            "    \n",
            "    \n",
            "    Number of classes\n",
            "        5\n",
            "    \n",
            "    \n",
            "    Args:\n",
            "        root: Directory where the datasets are saved.\n",
            "            Default: .data\n",
            "        split: split or splits to be returned. Can be a string or tuple of strings.\n",
            "            Default: ('train', 'test')\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KexfpCh3dG0B"
      },
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1G6NUiLdGxD",
        "outputId": "6ca6929c-e29e-4297-cbdc-81e861177893"
      },
      "source": [
        "train_iter = SogouNews(split='train')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "sogou_news_csv.tar.gz: 100%|██████████| 384M/384M [00:03<00:00, 103MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqWygUzhdGtH",
        "outputId": "f36ce2f0-ec4d-452f-d177-63fbeb5bc5c9"
      },
      "source": [
        "type(train_iter), len(train_iter), next(iter(train_iter))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torchtext.data.datasets_utils._RawTextIterableDataset,\n",
              " 450000,\n",
              " (4,\n",
              "  '2008 di4 qi1 jie4 qi1ng da3o guo2 ji4 che1 zha3n me3i nv3 mo2 te4  2008di4 qi1 jie4 qi1ng da3o guo2 ji4 che1 zha3n yu2 15 ri4 za4i qi1ng da3o guo2 ji4 hui4 zha3n zho1ng xi1n she4ng da4 ka1i mu4 . be3n ci4 che1 zha3n jia1ng chi2 xu4 da4o be3n yue4 19 ri4 . ji1n nia2n qi1ng da3o guo2 ji4 che1 zha3n shi4 li4 nia2n da3o che2ng che1 zha3n gui1 mo2 zui4 da4 di2 yi1 ci4 , shi3 yo4ng lia3o qi1ng da3o guo2 ji4 hui4 zha3n zho1ng xi1n di2 qua2n bu4 shi4 ne4i wa4i zha3n gua3n . yi3 xia4 we2i xia4n cha3ng mo2 te4 tu2 pia4n .'))"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSEm2IXGdGp2"
      },
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-h1da60dqtf"
      },
      "source": [
        "train_iter = SogouNews(split='train')\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "def yield_tokens(dataiter):\n",
        "  for (label,text) in dataiter:\n",
        "    yield tokenizer(text)\n",
        "\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=['<unk>'])\n",
        "vocab.set_default_index(vocab['<unk>'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XqoxLL-adqpG",
        "outputId": "fe482bfd-25e5-4050-ea69-489e0b9032bc"
      },
      "source": [
        "len(vocab)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "369441"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOCi6gpYdqme"
      },
      "source": [
        "text_pipeline = lambda x : vocab(tokenizer(x))\n",
        "label_pipeline = lambda x : int(x)-1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_943zxLcdqiI",
        "outputId": "df7ff7e9-0dd0-4223-e21e-53e9353c4401"
      },
      "source": [
        "text_pipeline(\"He is a country called India which is Big\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2361, 1481, 599, 7008, 10892, 7715, 2540, 1481, 3343]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9ptUoUZdqfk"
      },
      "source": [
        "def collate_batch(batch):\n",
        "  label_list, text_list, offsets = [],[],[0]\n",
        "  for (_label,_text) in batch:\n",
        "    label_list.append(label_pipeline(_label))\n",
        "    processed_text = torch.tensor(text_pipeline(_text),dtype=torch.int64)\n",
        "    text_list.append(processed_text)\n",
        "    offsets.append(processed_text.size(0))\n",
        "  label_list = torch.tensor(label_list,dtype=torch.int64)\n",
        "  text_list = torch.cat(text_list)\n",
        "  offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
        "  return label_list.to(device), text_list.to(device), offsets.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yiiBNdUuU7u"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "dataloader = DataLoader(train_iter, batch_size = 8, shuffle=False, collate_fn=collate_batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrxlDD4Pdqa7"
      },
      "source": [
        "from torch import nn\n",
        "\n",
        "class TextClassification(nn.Module):\n",
        "  def __init__(self, vocab_size, embed_size, num_classes):\n",
        "    super(TextClassification,self).__init__()\n",
        "    self.embed = nn.EmbeddingBag(vocab_size, embed_size, sparse=True)\n",
        "    self.fc = nn.Linear(embed_size, num_classes)\n",
        "    self.init_weights()\n",
        "  def init_weights(self):\n",
        "    init_range = 0.5\n",
        "    self.embed.weight.data.uniform_(-init_range,init_range)\n",
        "    self.fc.weight.data.uniform_(-init_range,init_range)\n",
        "    self.fc.bias.data.zero_()\n",
        "  def forward(self, text, offsets):\n",
        "    embedded = self.embed(text, offsets)\n",
        "    return self.fc(embedded)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zw8RrJ_RdqXN",
        "outputId": "9415785f-7af1-4b37-8c6d-2b86b1bb3e97"
      },
      "source": [
        "train_iter = SogouNews(split='train')\n",
        "vocab_size = len(vocab)\n",
        "embed_size = 64\n",
        "num_classes = len(set([label for (label,text) in train_iter]))\n",
        "model = TextClassification(vocab_size, embed_size, num_classes)\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TextClassification(\n",
              "  (embed): EmbeddingBag(369441, 64, mode=mean)\n",
              "  (fc): Linear(in_features=64, out_features=5, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6fURb5nuzx6",
        "outputId": "9036c9cf-a424-49fa-d9ba-c1c177d27a53"
      },
      "source": [
        "vocab_size, embed_size, num_classes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(369441, 64, 5)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeSwT82buzuJ"
      },
      "source": [
        "def train(dataloader):\n",
        "  model.train()\n",
        "  log_interval = 500\n",
        "  total_acc, total_count = 0,0\n",
        "  for idx,(label,text,offsets) in enumerate(dataloader):\n",
        "    optimizer.zero_grad()\n",
        "    predicted = model(text, offsets)\n",
        "    loss = criterion(predicted, label)\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1) # disuccees\n",
        "    optimizer.step()\n",
        "    total_acc += (predicted.argmax(1) == label).sum().item()\n",
        "    total_count += label.size(0)\n",
        "    if idx%log_interval == 0 and idx > 0:\n",
        "      print('| epochs {:3d} | {:5d}/{:5d} batches | accuracy{:8.3f}'.format(epoch, idx, len(dataloader), total_acc/total_count))\n",
        "      total_acc, total_count = 0,0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T80r6p79uzrD"
      },
      "source": [
        "def eval(dataloader):\n",
        "  model.eval()\n",
        "  total_acc, total_count = 0,0\n",
        "  with torch.no_grad():\n",
        "    for idx, (label,text,offsets) in enumerate(dataloader):\n",
        "      preds = model(text, offsets)\n",
        "      loss = criterion(preds, label)\n",
        "      total_acc += (preds.argmax(1) == label).sum().item()\n",
        "      total_count += label.size(0)\n",
        "  return total_acc/total_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HMEbx748xboU",
        "outputId": "de41df36-8337-4c8e-b101-2ff1930f3933"
      },
      "source": [
        "from torch.utils.data.dataset import random_split\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "EPOCHS = 5 #10\n",
        "BATCH_SIZE=64\n",
        "LR = 5\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(),lr=LR)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
        "total_accu = None\n",
        "train_iter, test_iter = SogouNews()\n",
        "train_dataset = to_map_style_dataset(train_iter)\n",
        "test_dataset = to_map_style_dataset(test_iter)\n",
        "num_train = int(len(train_dataset)*0.95)\n",
        "\n",
        "split_train, split_valid = random_split(train_dataset, [num_train, len(train_dataset)-num_train])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
        "valid_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle = True, collate_fn=collate_batch)\n",
        "\n",
        "for epoch in range(1,EPOCHS+1):\n",
        "  train(train_loader)\n",
        "  accu_val = eval(valid_loader)\n",
        "  if total_accu is not None and total_accu > accu_val:\n",
        "    scheduler.step()\n",
        "  else:\n",
        "      total_accu = accu_val\n",
        "  print('-' * 59)\n",
        "  print('| end of epoch {:3d} | '\n",
        "        'valid accuracy {:8.3f} '.format(epoch, accu_val))\n",
        "  print('-' * 59)  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epochs   1 |   500/ 7032 batches | accuracy   0.934\n",
            "| epochs   1 |  1000/ 7032 batches | accuracy   0.935\n",
            "| epochs   1 |  1500/ 7032 batches | accuracy   0.930\n",
            "| epochs   1 |  2000/ 7032 batches | accuracy   0.934\n",
            "| epochs   1 |  2500/ 7032 batches | accuracy   0.935\n",
            "| epochs   1 |  3000/ 7032 batches | accuracy   0.935\n",
            "| epochs   1 |  3500/ 7032 batches | accuracy   0.935\n",
            "| epochs   1 |  4000/ 7032 batches | accuracy   0.935\n",
            "| epochs   1 |  4500/ 7032 batches | accuracy   0.934\n",
            "| epochs   1 |  5000/ 7032 batches | accuracy   0.933\n",
            "| epochs   1 |  5500/ 7032 batches | accuracy   0.934\n",
            "| epochs   1 |  6000/ 7032 batches | accuracy   0.935\n",
            "| epochs   1 |  6500/ 7032 batches | accuracy   0.936\n",
            "| epochs   1 |  7000/ 7032 batches | accuracy   0.933\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   1 | valid accuracy    0.936 \n",
            "-----------------------------------------------------------\n",
            "| epochs   2 |   500/ 7032 batches | accuracy   0.935\n",
            "| epochs   2 |  1000/ 7032 batches | accuracy   0.934\n",
            "| epochs   2 |  1500/ 7032 batches | accuracy   0.935\n",
            "| epochs   2 |  2000/ 7032 batches | accuracy   0.937\n",
            "| epochs   2 |  2500/ 7032 batches | accuracy   0.936\n",
            "| epochs   2 |  3000/ 7032 batches | accuracy   0.937\n",
            "| epochs   2 |  3500/ 7032 batches | accuracy   0.935\n",
            "| epochs   2 |  4000/ 7032 batches | accuracy   0.934\n",
            "| epochs   2 |  4500/ 7032 batches | accuracy   0.939\n",
            "| epochs   2 |  5000/ 7032 batches | accuracy   0.936\n",
            "| epochs   2 |  5500/ 7032 batches | accuracy   0.935\n",
            "| epochs   2 |  6000/ 7032 batches | accuracy   0.933\n",
            "| epochs   2 |  6500/ 7032 batches | accuracy   0.938\n",
            "| epochs   2 |  7000/ 7032 batches | accuracy   0.934\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   2 | valid accuracy    0.934 \n",
            "-----------------------------------------------------------\n",
            "| epochs   3 |   500/ 7032 batches | accuracy   0.939\n",
            "| epochs   3 |  1000/ 7032 batches | accuracy   0.939\n",
            "| epochs   3 |  1500/ 7032 batches | accuracy   0.942\n",
            "| epochs   3 |  2000/ 7032 batches | accuracy   0.940\n",
            "| epochs   3 |  2500/ 7032 batches | accuracy   0.940\n",
            "| epochs   3 |  3000/ 7032 batches | accuracy   0.941\n",
            "| epochs   3 |  3500/ 7032 batches | accuracy   0.938\n",
            "| epochs   3 |  4000/ 7032 batches | accuracy   0.940\n",
            "| epochs   3 |  4500/ 7032 batches | accuracy   0.941\n",
            "| epochs   3 |  5000/ 7032 batches | accuracy   0.940\n",
            "| epochs   3 |  5500/ 7032 batches | accuracy   0.942\n",
            "| epochs   3 |  6000/ 7032 batches | accuracy   0.939\n",
            "| epochs   3 |  6500/ 7032 batches | accuracy   0.940\n",
            "| epochs   3 |  7000/ 7032 batches | accuracy   0.942\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   3 | valid accuracy    0.940 \n",
            "-----------------------------------------------------------\n",
            "| epochs   4 |   500/ 7032 batches | accuracy   0.941\n",
            "| epochs   4 |  1000/ 7032 batches | accuracy   0.943\n",
            "| epochs   4 |  1500/ 7032 batches | accuracy   0.937\n",
            "| epochs   4 |  2000/ 7032 batches | accuracy   0.942\n",
            "| epochs   4 |  2500/ 7032 batches | accuracy   0.943\n",
            "| epochs   4 |  3000/ 7032 batches | accuracy   0.940\n",
            "| epochs   4 |  3500/ 7032 batches | accuracy   0.939\n",
            "| epochs   4 |  4000/ 7032 batches | accuracy   0.940\n",
            "| epochs   4 |  4500/ 7032 batches | accuracy   0.942\n",
            "| epochs   4 |  5000/ 7032 batches | accuracy   0.942\n",
            "| epochs   4 |  5500/ 7032 batches | accuracy   0.941\n",
            "| epochs   4 |  6000/ 7032 batches | accuracy   0.941\n",
            "| epochs   4 |  6500/ 7032 batches | accuracy   0.939\n",
            "| epochs   4 |  7000/ 7032 batches | accuracy   0.941\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   4 | valid accuracy    0.941 \n",
            "-----------------------------------------------------------\n",
            "| epochs   5 |   500/ 7032 batches | accuracy   0.941\n",
            "| epochs   5 |  1000/ 7032 batches | accuracy   0.942\n",
            "| epochs   5 |  1500/ 7032 batches | accuracy   0.941\n",
            "| epochs   5 |  2000/ 7032 batches | accuracy   0.942\n",
            "| epochs   5 |  2500/ 7032 batches | accuracy   0.941\n",
            "| epochs   5 |  3000/ 7032 batches | accuracy   0.942\n",
            "| epochs   5 |  3500/ 7032 batches | accuracy   0.941\n",
            "| epochs   5 |  4000/ 7032 batches | accuracy   0.940\n",
            "| epochs   5 |  4500/ 7032 batches | accuracy   0.942\n",
            "| epochs   5 |  5000/ 7032 batches | accuracy   0.942\n",
            "| epochs   5 |  5500/ 7032 batches | accuracy   0.939\n",
            "| epochs   5 |  6000/ 7032 batches | accuracy   0.939\n",
            "| epochs   5 |  6500/ 7032 batches | accuracy   0.942\n",
            "| epochs   5 |  7000/ 7032 batches | accuracy   0.941\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   5 | valid accuracy    0.941 \n",
            "-----------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAYOp2Ixxbkz",
        "outputId": "f48ffb6f-c1d5-4a8d-d03f-8357ad49718d"
      },
      "source": [
        "print('Checking the results of test dataset.')\n",
        "accu_test = eval(test_loader)\n",
        "print('test accuracy {:8.3f}'.format(accu_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking the results of test dataset.\n",
            "test accuracy    0.936\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tETgjmXUxbhz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}